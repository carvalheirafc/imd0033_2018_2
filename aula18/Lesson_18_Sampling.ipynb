{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson 18 - Sampling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carvalheirafc/probabilidade_repository/blob/master/aula18/Lesson_18_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "H7B-hJbN8efL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - Introduction\n"
      ]
    },
    {
      "metadata": {
        "id": "UFfso72l8iVo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In previous lessons, we learned to perform basic data analysis and data visualization. We learned about some fundamental statistical metrics like the mean or the median, and we plotted histograms, bar graphs or line plots.\n",
        "\n",
        "In this step's lessons, we'll build on that knowledge, and we'll learn how to do better data analysis. First, **we'll go much deeper into the theory** behind what we've already learned. Second, we'll learn new and **more powerful statistical techniques** and metrics like **standard deviation, z-scores, confidence intervals, probability estimation, and hypothesis testing** (including A/B testing).\n",
        "\n",
        "In this first lesson, we begin with discussing the details around getting data for analysis, and continue with trying to understand the intricacies around how data is structured and measured. We'll then move on with learning techniques to organize and visualize relatively large amounts of data, which will make the process of finding patterns considerably less difficult.\n",
        "\n",
        "Below is a diagram describing the workflow we'll be focusing on throughout this first lesson.\n",
        "\n",
        "<img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=1gT_1Xs3Ts6A7p3y_wLetQrhOqN0GHmXG\"> \n",
        "\n",
        "**In this first lesson, our focus will be on the details around getting data for analysis.** As usual, we'll work with a real world data set. Before we dive into the technical details and start playing with the data, we begin with getting a sense about what statistics is.\n",
        "\n",
        "At this stage in our learning journey, a one-sentence definition of statistics would probably sound dull and be difficult to grasp. We'll avoid defining statistics that way, and **we'll discuss instead what sort of problems can be solved with statistics**. Understanding what challenges we can overcome using statistics should give us a good sense about what statistics is."
      ]
    },
    {
      "metadata": {
        "id": "YvNVkUHc8rP7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 - Solving Problems with Statistics\n",
        "\n",
        "\n",
        "Imagine you're managing a small tech company with 8 employees. At the end of the year, you piece together some data about your employees, with the intention of understanding better the state of your company. The data you collected is straightforward, and you can quickly make a few conclusions just by using a bit of arithmetics and logic.\n",
        "\n",
        "<img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1I9BNcNBVPXMsT9TFGzsuST2LOHGFkMfA\"> \n",
        "\n",
        "But years have gone by, and your business has grown into a successful company with 231 employees. You still want to get insights from data, but now you have so much of it that analyzing it has become difficult and inconvenient. As you continue to scale your company, analyzing data slowly gears toward becoming an impossible task.\n",
        "\n",
        "\n",
        "<img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1mN_CeAwRXW9l8z_PPegKsUKHBJAmyiXJ\"> \n",
        "\n",
        "This is an example of a problem we can solve with statistics. Using statistical techniques, we can organize, summarize, and visualize large amounts of data to find patterns that otherwise would remain hidden.\n",
        "\n",
        "<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=18k6rpBnDQHZd3bQeltcnHv6cxlD2GtdR\"> \n",
        "\n",
        "\n",
        "More years have gone by, and **now you run an international company with over 50000** employees. You've recently made a company-wide change which resulted in making the work of your employees more demanding. **Now you want to determine whether the employees have been impacted negatively in any significant way**. If this is true, then the change may backfire in the long run, and it'd be a good decision to revert the process while it's still possible.\n",
        "\n",
        "You reach out to your data analyst and ask for her opinion. She says that she can do a survey to collect data, and answer your question. Surveying over 50000 employees would be time-consuming and expensive, so you're being told that 100 people or so will be enough to survey to get an answer to your question.\n",
        "\n",
        "One week later, the analysis shows that people generally report they are less satisfied with their work compared to the last year (when the change hadn't been yet implemented). Also, the inability to balance work and personal life is the main reported cause of dissatisfaction. Your analyst also tells you that the decrease in satisfaction is significant, meaning that it's very unlikely to simply have happened by chance. Something must have caused the decrease, and that something is probably the major change you've done recently.\n",
        "\n",
        "This sort of scenario is very common in practice. As a data analyst, you'll often need to use a small set of data to answer questions about a much larger set of data.\n",
        "\n",
        "<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1-AIOVwx7TvSvZQ70iCdRtD9RVzKWN1GP\"> \n",
        "\n",
        "We'll learn ourselves throughout the statistics lessons how to use a small set of data to answer questions about a much larger set of data.\n",
        "\n",
        "Now we begin with discussing the details around collecting data, which is what the data analyst in our story did when she surveyed employees."
      ]
    },
    {
      "metadata": {
        "id": "o5LNC9URAKpt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 - Populations and Samples\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SYr0ASwBDHJk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data analyst in our previous example tried to answer whether people in the company are less satisfied at work compared to the previous year. Her question was about all the people inside the company. Yet she only selected a small group to answer the question.\n",
        "\n",
        "In statistics, the set of all individuals relevant to a particular statistical question is called a **population.** For our analyst's question, all the people inside the company were relevant. So the population in this case consisted from all the people in the company.\n",
        "\n",
        "A smaller group selected from a population is called a **sample**. When we select a smaller group from a population we do **sampling**. In our example, the data analyst took a sample of approximately 100 people from a population of over 50000 people.\n",
        "\n",
        "<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1e-KaAL-hlxSW1l2kWq5NNSiJ6AtrfE2E\"> \n",
        "\n",
        "Whether a set of data is a sample or a population depends on the question we're trying to answer. For our analyst's question, the population consisted of all the company members. But if we change the question, the same group of individuals can become a sample.\n",
        "\n",
        "For instance, if we tried to find out whether people at international companies are satisfied at work, then our group formed by over 50000 employees would become a sample. There are a lot of international companies out there, and ours is just one of them. The population (the set of all individuals relevant to this question) is made up of all the people working in all the international companies.\n",
        "\n",
        "<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1Su93y1eQdbhighuUNtTlFw3dLsZ0cOKK\"> \n",
        "\n",
        "\n",
        "Populations do not necessarily consist of people. Behavioral scientists, for instance, often try to answer questions about populations of monkeys, rats or other lab animals. In a similar way, other people try to answer questions about countries, companies, vegetables, soils, pieces of equipment produced in a factory, etc.\n",
        "\n",
        "The individual elements of a population or a sample go under many names. You'll often see the elements of a population referred to as individuals, units, events, observations. These are all used interchangeably and refer to the same thing: the individual parts of a population. When we use the term \"population individuals\", the population is not necessarily composed of people. \"Individuals\" here is a general term that could refer to people, needles, frogs, stars, etc.\n",
        "\n",
        "In the case of a sample, you'll often see this terminology used interchangeably: sample unit, sample point, sample individual, or sample observation.\n",
        "\n",
        "\n",
        "<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1SA18SfF65UNwMfrsBjboMkh92OgJ9uSw\"> \n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- Now it's our turn to play the data analyst. **We collected data about the salary of all the individuals in the company working in IT roles**. Based on this data, we want to answer a series of questions. Depending on the question, our data is either a sample or a population. Identify which is the case, and assign to the corresponding variable the string **'population'** or **'sample'**. Here are the questions we need to answer:\n",
        "\n",
        "  - What's the average salary of the individuals in our company working in IT roles? (Assign either **'population'** or **'sample'** to the variable **question1**.)\n",
        "  - What's the proportion of individuals in the IT department having salaries under 60000 (Assign either **'population'** or **'sample'** to the variable **question2.**)\n",
        "  - What's the minimum salary in the entire company? (Assign either **'population'** or **'sample'** to the variable **question3.**)\n",
        "  - What's the minimum salary in the IT department of our company? (Assign either **'population'** or **'sample'** to the variable **question4.**)\n",
        "  - What's the proportion of salaries under 20000 in the entire company? (Assign either **'population'** or **'sample'** to the variable **question5.**)"
      ]
    },
    {
      "metadata": {
        "id": "vkDmG3miDeRX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Polpulation def.: The set of all individuals relevant to a particular statistical\n",
        "# Sample def.: A smaller group selected from a population\n",
        "\n",
        "question1 = ''\n",
        "question2 = ''\n",
        "question3 = ''\n",
        "question4 = ''\n",
        "question5 = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QAtKJLhtIJCG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4 - Sampling Error"
      ]
    },
    {
      "metadata": {
        "id": "HxjyFHDyNuaQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For every statistical question we want to answer, we should try to use the population. In practice, that's not always possible because the populations of interest usually vary from large to extremely large. Also, getting data is generally not an easy task, so small populations often pose problems too.\n",
        "\n",
        "These problems can be solved by sampling from the population that interests us. Although not as good as working with the entire population, working with a sample is the next best thing we can do.\n",
        "\n",
        "When we sample, the data we get might be more or less similar to the data in the population. For instance, let's say we know that the average salary in our company is 34500 dollars, and the proportion of women is 60%. We take two samples and find these results:\n",
        "\n",
        "<img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1mQSKBS8sB53KnaM4HYW8Og22HNv5e3El\">\n",
        "\n",
        "\n",
        "As you can see, the metrics of the two samples are different than the metrics of the population. A sample is by definition an incomplete set of data for the question we're trying to answer. For this reason, there's almost always some difference between the metrics of a population and the metrics of a sample. This difference can be seen as an error, and because it's the result of sampling, it's called **sampling error**.\n",
        "\n",
        "A metric specific to a population is called a **parameter**, while one specific to a sample is called a **statistic**. In our example above, the average salary of all the employees is a parameter because it's a metric that describes the entire population. The average salaries from our two samples are examples of statistics because they only describe the samples.\n",
        "\n",
        "Another way to think of the concept of the sampling error is as the difference between a parameter and a statistic:\n",
        "\n",
        "$$\n",
        "\\text{sampling error} = parameters - statistics\n",
        "$$\n",
        "\n",
        "At this point in the lesson, we'll move from the tech company example to working with a real world data set. Our first challenge will be to measure sampling error using this data set.\n",
        "\n",
        "The data set is about basketball players in WNBA (Women's National Basketball Association), and contains general information about players, along with their metrics for the season 2016-2017. The data set was put together by Thomas De Jonghe, and can be downloaded from [Kaggle](https://www.kaggle.com/jinxbe/wnba-player-stats-2017), where you can also find useful documentation for the data set.\n",
        "\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "- Get familiar with the data set.\n",
        "  - Print the first five rows using **DataFrame.head()** and the last five rows with **DataFrame.tail()**.\n",
        "  - Find the number of rows and columns using **DataFrame.shape**.\n",
        "  - Learn about each column from the [documentation](https://www.kaggle.com/jinxbe/wnba-player-stats-2017). You can also find useful documentation in this [glossary](https://www.basketball-reference.com/about/glossary.html) and on WNBA's [official page](http://www.wnba.com/stats/player-stats/).\n",
        "\n",
        "- Take one measure of the sampling error.\n",
        "  - Use the **Games Played** column to find the **maximum** number of **games played** by a player in the season 2016-2017. The data set contains all the players that had at least one game, so it's a population relative to our question. Find this parameter, and assign the result to a variable named **parameter.**\n",
        "  - Using the **Series.sample()** method, sample randomly 30 players from the population, and assign the result to a variable named **sample.**\n",
        "  - When calling **Series.sample()**, use the the argument **random_state = 1**. This makes your results reproducible and helps us with the answer checking (we'll discuss more about this later).\n",
        "  - Find the maximum number of games using the **sample**, and assign the result to a variable named **statistic**.\n",
        "  - Measure the **sampling error**, and assign the result to a variable named **sampling_error.**\n"
      ]
    },
    {
      "metadata": {
        "id": "PLl4FkjHN82d",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "65b5f337-49ed-4b4c-cc6d-4f2ee595dc27"
      },
      "cell_type": "code",
      "source": [
        "# Uploading files from your local file system\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-44a0b143-6b1e-461a-b62b-d82026593287\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-44a0b143-6b1e-461a-b62b-d82026593287\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving WNBA Stats.csv to WNBA Stats.csv\n",
            "User uploaded file \"WNBA Stats.csv\" with length 20793 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lRw0LzHF-_BC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here\n",
        "import pandas as pd\n",
        "\n",
        "# see all columns\n",
        "pd.set_option('display.max_columns', 500)\n",
        "\n",
        "# read the dataset\n",
        "wnba = pd.read_csv('WNBA Stats.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BQOi2Te8ATzA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(wnba.head(5))\n",
        "print(wnba.tail(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41O3_AbKAWqo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wnba.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8NJ_kF0AoUr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29470c6d-cb46-42de-d361-8485a051d7e2"
      },
      "cell_type": "code",
      "source": [
        "wnba['Games Played'].max()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "56FS9Ei7UpMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2893b498-44f3-42e4-e3fb-8910276f8cb9"
      },
      "cell_type": "code",
      "source": [
        "# Using Sample Series\n",
        "\n",
        "wnba.sample(n=30, random_state=1)['Games Played'].max()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "sKfyt00SVbVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Sample Error\n",
        "wnba['Games Played'].max() - wnba.sample(n=50, random_state=1)['Games Played'].max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T8Ja8OuYroR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5.0 - Simple Random Sampling"
      ]
    },
    {
      "metadata": {
        "id": "8mjE3DY8ZSvO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When we sample we want to minimize the sampling error as much as possible. We want our sample to mirror the population as closely as possible.\n",
        "\n",
        "If we sampled to measure the mean height of adults in the US, we'd like our sample statistic (sample mean height) to get as close as possible to the population's parameter (population mean height). For this to happen, we need the individuals in our sample to form a group that is similar in structure with the group forming the population.\n",
        "\n",
        "The US adult population is diverse, made of people of various heights. If we sampled 100 individuals from various basketball teams, then we'd almost certainly get a sample whose structure is significantly different than that of the population. As a consequence, we should expect a large sampling error (a large discrepancy between our sample's statistic (sample mean height) and the population's parameter (population mean height)).\n",
        "\n",
        "In statistical terms, we want our samples to be **representative** of their corresponding populations. If a sample is representative, then the sampling error is low. The more representative a sample is, the smaller the sampling error. The less representative a sample is, the greater the sampling error.\n",
        "\n",
        "<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=17nu2_nVUaiF-W8uyEas6kjPfVJDXtImL\">\n",
        "\n",
        "\n",
        "To make our samples representative, we can try to give every individual in the population an equal chance to be selected in our samples. We want a very tall individual to have the same chance as being selected as an individual having a medium or short height. To give every individual an equal chance of being picked, we need to sample **randomly.**\n",
        "\n",
        "One way to perform random sampling is to generate random numbers and use them to select a few sample units from the population. In statistics, this sampling method is called **simple random sampling**, and it's often abbreviated as **SRS**.\n",
        "\n",
        "\n",
        "<img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1HpBZT_9nh8R-A5lRvQ0vif2SSOytZ31X\">\n",
        "\n",
        "\n",
        "In our previous exercise, we used **Series.sample()** to sample. This method performs simple random sampling by generating an array of random numbers, and then using those numbers to select values from a **Series** at the indices corresponding to those random numbers. The method can be also extended for DataFrame [objects](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html?highlight=sample#pandas.DataFrame.sample), where random rows or columns can be sampled.\n",
        "\n",
        "When we use the **random_state** parameter, like we did in the previous exercise with **Series.sample(30, random_state = 1)**, we make the generation of random numbers predictable. This is because **Series.sample()** uses a [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) under the hood. A pseudorandom number generator uses an initial value to generate a sequence of numbers that has properties similar to those of a sequence that is truly random. With **random_state** we specify that initial value used by the pseudorandom number generator.\n",
        "\n",
        "If we want to generate a sequence of five numbers using a pseudorandom generator, and begin from an initial value of 1, we'll get the same five numbers no matter how many times we run the code. If we ran **wnba['Games Played'].sample(5, random_state = 1)** we'd get the same sample every time we run the code.\n",
        "\n",
        "Pseudorandom number generators are of great use in scientific research where [reproducible](https://en.wikipedia.org/wiki/Reproducibility) work is necessary. In our case, pseudorandom number generators allow us to work with the same samples as you do in the exercises, which allows in turn for a meaningful answer checking.\n",
        "\n",
        "\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "Let's visualize the discrepancy between a **parameter** and its corresponding **statistics** in the case of **simple random sampling**.\n",
        "\n",
        "- Using simple random sampling, take **100 samples** from our WNBA dataset, and for each sample measure the average points scored by a player during the 2016-2017 season. For each of the 100 iterations of a for loop:\n",
        "  - Sample 10 values from the **PTS** column.\n",
        "  - Compute the **mean** of this sample made of 10 values from the **PTS** column, and append the result to a list.\n",
        "  - To make your results reproducible, vary the **random_state** parameter of the **sample()** method with values between 0 and 99. For the first iteration of the for loop, **random_state** should equal 0, for the second iteration should equal 1, for the third should equal 2, and so on.\n",
        "- Display the discrepancy between the parameter of interest (the **mean** of the **PTS** column) and the statistics obtained in the previous step.\n",
        "  - Using **plt.scatter()**, display all the 100 sample means using a scatter plot. For the x-axis, use integers from 1 to 100 to designate the sample number. Use the y-axis for the sample means.\n",
        "  - Using **plt.axhline()**, draw a horizontal line that represents the average number of points in the population."
      ]
    },
    {
      "metadata": {
        "id": "kdo4-YAfg9bH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FZYVe9G_eygo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.0 The Importance of Sample Size"
      ]
    },
    {
      "metadata": {
        "id": "2SPlkEXWhHfC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the scatter plot in the last screen, we can notice that the sample means vary a lot around the population mean. With a minimum sample mean of 115 points, a maximum of 301.4, and a population mean of roughly 201.8, we can tell that the sampling error is quite large for some of the cases.\n",
        "\n",
        "\n",
        "<center><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1ocEuzFUJ16saCCA6L8u1ACh3C--8wom6\"></center>\n",
        "\n",
        "\n",
        "Because sample means vary a lot around the population mean, there's a good chance we get a sample that is not representative of the population:\n",
        "\n",
        "<center><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1_oaVyrm9xbXqCeDjmDDYykgnMI-YtmtA\"></center>\n",
        "\n",
        "This problem can be solved by increasing the sample size. As we increase the sample size, the sample means vary less around the population mean, and the chances of getting an unrepresentative sample decrease.\n",
        "\n",
        "In our last exercise we took 100 samples, and each had a sample size of 10 units. This is what happens when we repeat the procedure, but increase the size of the samples:\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=16qgq9xJscpVHG2yKBv6bW9m9ELNDGYBg\"></center>\n",
        "\n",
        "\n",
        "We can easily see how sample means tend to vary less and less around the population mean as we increase the sample size. From this observation we can make two conclusions:\n",
        "\n",
        "- **Simple random sampling** is not a reliable sampling method when the sample size is small. Because sample means vary a lot around the population mean, there's a good chance we'll get an unrepresentative sample.\n",
        "- When we do simple random sampling, we should try to get a sample that is as large as possible. A large sample decreases the variability of the sampling process, which in turn decreases the chances that we'll get an unrepresentative sample.\n",
        "\n",
        "<center><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1Q9J3hcG_fSsKIxhD8Gs3sZx7BXJ7xq1n\"></center>\n"
      ]
    },
    {
      "metadata": {
        "id": "a5-GVtFzhT5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 7. Stratified Sampling\n"
      ]
    },
    {
      "metadata": {
        "id": "8t_lI7pLj_6g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because simple random sampling is entirely random, it can leave out certain population individuals that are of great interest to some of the questions we may have.\n",
        "\n",
        "For example, players in basketball play in different positions on the court. The metrics of a player (number of points, number of assists, etc.) depend on their position, and we might want to analyze the patterns for each individual position. If we perform simple random sampling, there's a chance that some categories won't be included in our sample. In other words, it's not guaranteed that we'll have a representative sample that has observations for every position we want to analyze.\n",
        "\n",
        "There are five unique positions in our data set:\n",
        "\n",
        "```python\n",
        ">> wnba['Pos'].unique()\n",
        "array(['F', 'G/F', 'G', 'C', 'F/C'], dtype=object)\n",
        "```\n",
        "\n",
        "Let's decipher quickly each abbreviation:\n",
        "\n",
        "| Abbreviation | Full name      |\n",
        "|--------------|----------------|\n",
        "| F            | Forward        |\n",
        "| G            | Guard          |\n",
        "| C            | Center         |\n",
        "| G/F          | Guard/Forward  |\n",
        "| F/C          | Forward/Center |\n",
        "\n",
        "\n",
        "The downside of simple random sampling is that it can leave out individuals playing in a certain position on the field. Visually, and on a smaller scale, this is what could happen:\n",
        "\n",
        "\n",
        "<center><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1U4PL7w8sJfk1b-PNcRbBu4bQh8kYNwrZ\"></center>\n",
        "\n",
        "\n",
        "To ensure we end up with a sample that has observations for all the categories of interest, we can change the sampling method. We can organize our data set into different groups, and then do simple random sampling for every group. We can group our data set by player position, and then sample randomly from each group.\n",
        "\n",
        "Visually, and on a smaller scale, we need to do this:\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1mImCHKEUeoL7AzB8G3rRmCg3gs22RZ00\"></center>\n",
        "\n",
        "\n",
        "This sampling method is called **stratified sampling**, and each stratified group is also known as a **stratum.**\n",
        "\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "Perform stratified sampling: stratify the data set by player position, and then do simple random sampling on every stratum. At the end, use the sample to find which position has the greatest number of points per game.\n",
        "\n",
        "- Create a new column which describes the **number of points** a player scored **per game** during the season. The number of total points a player scored the entire season is stored in the **PTS** column, and the number of games played in the **Games Played** column. Give the new column name to **Pts_per_game**.\n",
        "- Stratify the **wnba** data set by player position. The **Pos** column describes a player's position on the field. Assign each stratum to a different variable.\n",
        "```python\n",
        "stratum_G = wnba[wnba.Pos == 'G']\n",
        "```\n",
        "- Loop through the strata, and for each stratum:\n",
        "  - Sample 10 observations using simple random sampling (set **random_state = 0**).\n",
        "  - Find the mean points per game using the sample. Use the new column you've created earlier (**Pts_per_game**).\n",
        "  - Find a way to store the mean along with its corresponding position. **You can use a dictionary.**\n",
        "- Find the position that has the greatest number of points per game, and assign its name to a variable named **position_most_points.**\n",
        "  - To find the dictionary key that has the greatest dictionary value, you can use this [technique](https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary#280156)."
      ]
    },
    {
      "metadata": {
        "id": "7MGXk55Vm82i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DSkpFfBxnEva",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 8.0 . Proportional Stratified Sampling"
      ]
    },
    {
      "metadata": {
        "id": "3Xlcik59o1ih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Earlier in this lesson we performed simple random sampling 100 times on the original data set, and for each sample we computed the mean number of total points a player scores in a season. The problem is that the number of total points is influenced by the number of games played, which ranges from 2 to 32:\n",
        "\n",
        "```python\n",
        ">> wnba['Games Played'].min()\n",
        "2\n",
        "\n",
        ">> wnba['Games Played'].max()\n",
        "32\n",
        "```\n",
        "\n",
        "Approximately 72.7% of the players had more than 23 games for the 2016-2017 season, which means that the mean of the total points is probably influenced by this category of players who played a lot of games. Let's take a look at the other percentages too:\n",
        "\n",
        "```python\n",
        ">> wnba['Games Played'].value_counts(bins = 3, normalize = True) * 100\n",
        "(22.0, 32.0]     72.727273\n",
        "(12.0, 22.0]     18.181818\n",
        "(1.969, 12.0]     9.090909\n",
        " \n",
        "Name: Games Played, dtype: float64\n",
        " ```\n",
        " \n",
        "As a side note on the output above, (1.969, 12.0], (12.0, 22.0] and (22.0, 32.0] are number intervals. The ( character indicates that the beginning of the interval is not included, and the ] indicates that the endpoint is included. For example, (22.0, 32.0] means that 22.0 isn't included, while 32.0 is, and the interval contains this array of numbers: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32].\n",
        "\n",
        "Getting back to our discussion, when we compute the mean of the total points using the population (the entire data set), the mean will probably be signficantly influenced by those 72.7% players who played more than 23 games. However, when we sample randomly, we can end up with a sample where the proportions are different than in the population.\n",
        "\n",
        "For instance, we might end up with a sample where only 2% of the players played more than 23 games. This will result in a sample mean which underestimates the population mean. Or we could have a sample where more than 95% of the players had 23 games in the 2016-2017 season. This will result in overestimating the population mean. This scenario of under or over estimation is common for small samples.\n",
        "\n",
        "<center><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=14m8dpqQZ3kbGHeJDWXiIvrl3Rf5xzYr6\"></center>\n",
        "\n",
        "One solution to this problem is to use stratified sampling while being mindful of the proportions in the population. We can stratify our data set by the number of games played, and then sample randomly from each stratum a proportional number of observations.\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1J8vHUBN1EEEzvoQB_KS6TpUOdIUNG_JZ\"></center>\n",
        "\n",
        "In the diagram above, we can see that from a population of 20 individuals:\n",
        "\n",
        "- 14 individuals played more than 22 games.\n",
        "- 4 individuals played between 13 and 22 games.\n",
        "- 2 individuals played below 13 games.\n",
        "\n",
        "Transforming these figures to percentages, 70% of the individuals played more than 22 games, 20% played between 13 and 22 games, and 10% played below 13 games. Because we sampled proportionally, the same percentages (70%, 20%, 10%) are preserved in the sample (even though the absolute values are different): 70% played more than 22 games, 20% played between 13 and 22 games, and 10% played below 13 games.\n"
      ]
    },
    {
      "metadata": {
        "id": "nrwplt0hpOvV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "under_12 = wnba[wnba['Games Played'] <= 12]\n",
        "btw_13_22 = wnba[(wnba['Games Played'] > 12) & (wnba['Games Played'] <= 22)]\n",
        "over_23 = wnba[wnba['Games Played'] > 22]\n",
        "\n",
        "proportional_sampling_means = []\n",
        "\n",
        "for i in range(100):\n",
        "    sample_under_12 = under_12['PTS'].sample(1, random_state = i)\n",
        "    sample_btw_13_22 = btw_13_22['PTS'].sample(2, random_state = i)\n",
        "    sample_over_23 = over_23['PTS'].sample(7, random_state = i)\n",
        "    \n",
        "    final_sample = pd.concat([sample_under_12, sample_btw_13_22, sample_over_23])\n",
        "    proportional_sampling_means.append(final_sample.mean())\n",
        "    \n",
        "plt.scatter(range(1,101), proportional_sampling_means)\n",
        "plt.axhline(wnba['PTS'].mean(),color=\"red\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JGBV1tijpRu2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 9.0 Choosing the Right Strata\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DMtws2JeqZMf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You might not have been very impressed by what we've just got with sampling proportionally. The variability of the sampling was quite large, and many sample means were unrepresentative, being far from the population mean. In fact, this sampling method doesn't seem to perform better than simple random sampling:\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1uoEz0OppoAB-I7-IK9r4A4SfQizf4UHZ\"></center>\n",
        "\n",
        "The poor performance is caused by a bad choice of strata. We stratified the data by the number of games played, but this isn't a good approach. A player is considered as having played one game even if she only played for one or two minutes. But others play 30 or 40 minutes, and they're still considered as having played one game.\n",
        "\n",
        "It makes more sense to stratify the data by number of minutes played, rather than by number of games played. The minutes played are a much better indicator of how much a player scored in a season than the number of games played.\n",
        "\n",
        "Our data set contains the total amount of minutes a player had for the entire season. If we make strata based on minutes played, and then sample proportionally using stratified sampling, we get something visibly better than simple random sampling (especially in terms of variability):\n",
        "\n",
        "<center><img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=1dBdLH2qGGGdu4mph-2GzCVfuBoizO_Tq\"></center>\n",
        "\n",
        "\n",
        "We increased the sample size to 12 so that we can do a better proportional sampling for the strata organized by minutes played.\n",
        "\n",
        "Here are a few guidelines for choosing good strata:\n",
        "\n",
        "**1. Minimize the variability within each stratum.**\n",
        "\n",
        "For instance, avoid having in the same stratum a player that has scored 10 points and a player that has scored 500. If the variability is high, it might be a sign that you either need a more granular stratification (need more strata), or you need to change the criterion of stratification (an example of criterion is minutes played).\n",
        "\n",
        "**2. Maximize the variability between strata.**\n",
        "\n",
        "Good strata are different from one another. If you have strata that are similar to one another with respect to what you want to measure, you might need a more granular stratification, or to change the stratification criterion. In the previous screen, stratifying the data by games played resulted in strata that weren't too different from each other with respect to the distribution of the total points. We managed to increase the variability between strata by changing the criterion of stratification to minutes played.\n",
        "\n",
        "**3. The stratification criterion should be strongly correlated with the property you're trying to measure.**\n",
        "\n",
        "For instance, the column describing minutes played (the criterion) should be strongly correlated with the number of total points (property we want to measure). We've covered briefly the concept of correlation in the pandas lessons, and we'll cover it again later in these statistics lessons, so don't worry if the concept of correlation doesn't make much sense to you now.\n",
        "\n",
        "We've left the code editor open for you to try to experiment with the different sampling methods we've learned so far. One thing you can try is to replicate the last graph above. You can then play with sample sizes, and try to get insights into how variability and sampling error change."
      ]
    },
    {
      "metadata": {
        "id": "SK0f8teoxLIu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wnba['MIN'].value_counts(bins = 3, normalize = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J9c_GCA3yTrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 10.0 Cluster Sampling"
      ]
    },
    {
      "metadata": {
        "id": "TgIYF03qyp-R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data set we've been working with was scraped from the [WNBA's website](http://www.wnba.com/stats/player-stats/#?Season=2017&SeasonType=Regular%20Season&PerMode=Totals). The website centralizes data on basketball games and players in WNBA. Let's suppose for a moment that there weren't such a site, and the data would be scattered instead across each individual team's website. There are twelve unique teams in our data set, which means we'd have to scrape twelve different websites, each requiring its own scraping script.\n",
        "\n",
        "This scenario is quite common in the data science workflow: you want to answer some questions about a population, but the data is scattered in such a way that data collection is either time-consuming, either close to impossible. For instance, let's say you want to analyze how people review and rate movies as a function of movie budget. There are a lot of websites out there that can help with data collection, but how can you go about so that you can spend one day or two on getting the data you need, rather than one month or two?\n",
        "\n",
        "One way is to list all the data sources you can find, and then randomly pick only a few of them to collect data from. Then you can sample individually each of the sources you've randomly picked. This sampling method is called **cluster sampling**, and each of the individual data sources is called a **cluster**.\n",
        "\n",
        "<center><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1hS05bjdUFpy9xoLEqDeaoeFyb-Z9zGkZ\"></center>\n",
        "\n",
        "In our case, we'd first list all the possible data sources. Assuming that all the teams in our data set have a website where we can take data from, we end up with this list of clusters (each team's website is considered a cluster) :\n",
        "\n",
        "\n",
        "```python\n",
        ">> wnba['Team'].unique()\n",
        "array(['DAL', 'LA', 'CON', 'SAN', 'MIN', 'SEA', 'PHO', 'CHI', 'WAS', 'NY',\n",
        "       'ATL', 'IND'], dtype=object)\n",
        " ```\n",
        " \n",
        " Then we need to find a way to pick randomly a few clusters from our listing. There are many ways to do that, but the important thing to keep in mind is that we should avoid picking a cluster twice. Here's one way to sample four clusters randomly:\n",
        "\n",
        "```python\n",
        " >> pd.Series(wnba['Team'].unique()).sample(4, random_state = 0)\n",
        "6     PHO\n",
        "11    IND\n",
        "4     MIN\n",
        "10    ATL\n",
        "dtype: object\n",
        "```\n",
        "\n",
        "Once we picked the clusters, we move to collecting the data. We can collect all the data from each cluster, but we can also perform sampling on each. It's actually possible to use different sampling methods for different clusters. For instance, we can use stratified sampling on the first two clusters, and simple random sampling on the other two.\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "Let's simulate a cluster sampling on our data set.\n",
        "\n",
        "- Pick four team clusters randomly using the technique we've learned (use **random_state = 0**).\n",
        "- Collect the data from each cluster without sampling the clusters. Create a new DataFrame object that stores the data collected from all clusters.\n",
        "- Use the data collected to estimate the mean for the following player attributes:\n",
        "  - Height;\n",
        "  - Age;\n",
        "  - BMI;\n",
        "  - Total points.\n",
        "- Finally, measure the sampling error of your estimates, and assign the errors to the following variables: **sampling_error_height**, **sampling_error_age**, **sampling_error_BMI**, **sampling_error_points**.\n",
        " \n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "QFjmf1F-Bt52",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pick four team clusters\n",
        "clusters = pd.Series(wnba['Team'].unique()).sample(4, random_state = 0)\n",
        "\n",
        "# create a new dataframe\n",
        "sample = pd.DataFrame()\n",
        "\n",
        "# generate the sample with clusters data\n",
        "for cluster in clusters:\n",
        "    data_collected = wnba[wnba['Team'] == cluster]\n",
        "    sample = sample.append(data_collected)\n",
        "    \n",
        "# put your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e7Lrgp0Bzymv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 11.0 Sampling in Data Science Practice"
      ]
    },
    {
      "metadata": {
        "id": "VYf3VXpW0CmE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So far, we've explored a few scenarios where sampling can be useful. There are more situations, however, where a data scientist can use sampling, and we discuss a few in this section.\n",
        "\n",
        "Let's say you work for an e-commerce company that has a table in a database with more than 10 million rows of online transactions. The marketing team asks you to analyze the data and find categories of customers with a low buying rate, so that they can target their marketing campaigns at the right people. Instead of working with more than 10 million rows at each step of your analysis, you can save a lot of code running time by sampling several hundred rows, and perform your analysis on the sample. You can do a simple random sampling, but if you're interested in some categories beforehand, it might be a good idea to use stratified sampling.\n",
        "\n",
        "Let's consider a different situation. It could be that you need to collect data from an API that either has usage limit, or is not free. In this case, you are more or less forced to sample. Knowing how and what to sample can be of great use.\n",
        "\n",
        "Another common use case of sampling is when the data is scattered across different locations (different websites, different databases, different companies, etc.). As we've discussed in the previous screen, cluster sampling would be a great choice in such a scenario.\n",
        "\n",
        "Sampling is a vast topic in statistics, and there are other sampling methods besides what we've discussed.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Sampling_(statistics)#Sampling_methods"
      ]
    },
    {
      "metadata": {
        "id": "riX6SMYT0LuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 12.0 Descriptive and Inferential Statistics"
      ]
    },
    {
      "metadata": {
        "id": "RA-V3x6e0mBH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Practical statistical analysis revolves entirely around the distinction between a population and a sample. When we're doing statistics in practice, our goal is either to describe a sample or a population, or to use a sample to draw conclusions about the population it belongs (or a mix of these two goals).\n",
        "\n",
        "When we describe a sample or a population (by measuring averages, proportions, and other metrics; by visualizing properties of the data through graphs; etc.), we do descriptive statistics.\n",
        "\n",
        "When we try to use a sample to draw conclusions about a population, we do inferential statistics (we infer information from the sample about the population).\n",
        "\n",
        "<center><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1UavGgVsCaegcbBe5Dn0zq2o7vPf-lOQ7\"></center>\n"
      ]
    },
    {
      "metadata": {
        "id": "ooRn1i8E0pto",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 13.0 Next steps\n"
      ]
    },
    {
      "metadata": {
        "id": "GoltkoZ31FWC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Throughout this mission, we discussed the details around collecting data for our analysis, and completed the first part of the workflow we'll cover in this course.\n",
        "\n",
        "<center><img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=1VHgybLpQdxMkbZ7HZCYPl1uAnS5UErF7\"></center>\n",
        "\n",
        "Next in our statistics journey, we'll learn what variables are in statistics, how data sets are structured as collection of variables, and how each variable is measured."
      ]
    }
  ]
}